{
    "models": [
        {
            "name": "Qwen3",
            "profiles": [
                {
                    "name": "thinking",
                    "command": "./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja -fa --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift",
                    "references": [
                        "https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-cli",
                        "https://huggingface.co/Qwen/Qwen3-235B-A22B#switching-between-thinking-and-non-thinking-mode"
                    ]
                },
                {
                    "name": "non thinking",
                    "command": "./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja -fa --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0 -c 40960 -n 32768 --no-context-shift",
                    "references": [
                        "https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-cli",
                        "https://huggingface.co/Qwen/Qwen3-235B-A22B#switching-between-thinking-and-non-thinking-mode"
                    ]
                }
            ]
        },
        {
            "name": "QwQ",
            "profiles": [
                {
                    "name": "chat",
                    "command": "./llama-cli --model unsloth-QwQ-32B-GGUF/QwQ-32B-Q4_K_M.gguf --ctx-size 16384 --temp 0.6 --repeat-penalty 1.1 --dry-multiplier 0.5 --min-p 0.01 --top-k 40 --top-p 0.95 --samplers \"top_k;top_p;min_p;temperature;dry;typ_p;xtc\"",
                    "references": [
                        "https://modelscope.cn/models/Qwen/QwQ-32B",
                        "https://huggingface.co/Qwen/QwQ-32B",
                        "https://huggingface.co/unsloth/QwQ-32B-GGUF"
                    ]
                }
            ]
        },
        {
            "name": "Llama-4",
            "profiles": [
                {
                    "name": "chat",
                    "command": "./llama-cli --model unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/Llama-4-Scout-17B-16E-Instruct-UD-IQ2_XXS.gguf --ctx-size 16384 -ot \".ffn_.*_exps.=CPU\" --temp 0.6 --min-p 0.01 --top-p 0.9",
                    "references": [
                        "https://www.llama.com/docs/llama-everywhere/running-meta-llama-on-linux/",
                        "https://docs.unsloth.ai/basics/llama-4-how-to-run-and-fine-tune"
                    ]
                }
            ]
        },
        {
            "name": "Gemma3",
            "profiles": [
                {
                    "name": "chat",
                    "command": "./llama-cli --model bartowski/google_gemma-3-27b-it-qat-GGUF/gemma-3-27b-it-Q4_K_M.gguf --ctx-size 16384 --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95",
                    "references": [
                        "https://ollama.com/library/gemma3/blobs/3116c5225075",
                        "https://docs.unsloth.ai/basics/gemma-3-how-to-run-and-fine-tune"
                    ]
                }
            ]
        },
        {
            "name": "Phi-4-reasoning",
            "profiles": [
                {
                    "name": "chat",
                    "command": "./llama-cli --model bartowski/microsoft_Phi-4-reasoning-plus-GGUF/microsoft_Phi-4-reasoning-plus-Q4_K_M.gguf --ctx-size 32768 --temp 0.8 --top-k 50 --top-p 0.95",
                    "references": [
                        "https://huggingface.co/microsoft/Phi-4-reasoning"
                    ]
                }
            ]
        },
        {
            "name": "DeepCoder-Preview",
            "profiles": [
                {
                    "name": "chat",
                    "command": "./llama-cli --model bartowski/agentica-org_DeepCoder-14B-Preview-GGUF/agentica-org_DeepCoder-14B-Preview-Q6_K.gguf --ctx-size 64000 --temp 0.6 --top-p 0.95",
                    "references": [
                        "https://huggingface.co/agentica-org/DeepCoder-14B-Preview"
                    ],
                    "notes": [
                        "the model performs best with max_tokens set to at least 64000"
                    ]
                }
            ]
        },
        {
            "name": "EXAONE-Deep",
            "profiles": [
                {
                    "name": "chat",
                    "command": "./llama-cli --model bartowski/LGAI-EXAONE_EXAONE-Deep-32B-GGUF/LGAI-EXAONE_EXAONE-Deep-32B-Q4_K_M.gguf --ctx-size 32768 --temp 0.6 --top-p 0.95",
                    "references": [
                        "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-32B"
                    ]
                }
            ]
        },
        {
            "name": "Mistral-Small-24B-Instruct-2501",
            "profiles": [
                {
                    "name": "chat",
                    "command": "./llama-cli --model bartowski/Mistral-Small-24B-Instruct-2501-GGUF/Mistral-Small-24B-Instruct-2501-Q4_K_M.gguf --ctx-size 32768 --temp 0.15",
                    "references": [
                        "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
                    ]
                }
            ]
        }
    ]
}